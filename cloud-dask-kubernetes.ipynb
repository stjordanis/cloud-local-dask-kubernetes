{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run dask-kubernetes clusters on a cloud from your Laptop\n",
    "\n",
    "You can run your Jupyter Notebook locally, and connect easily to a remote `dask-kubernetes` cluster\n",
    "on a cloud-based Kubernetes Cluster with the help of [kubefwd](https://kubefwd.com/). This notebook\n",
    "will show you an example of how to do so. While this example is a Jupyter Notebook, the code will work\n",
    "any local python medium - REPL, IDE (vscode), or just plain ol' `.py` files\n",
    "\n",
    "Latest executable version of this notebook can be found in [this repository](https://github.com/yuvipanda/cloud-local-dask-kubernetes)\n",
    "\n",
    "## Credit\n",
    "\n",
    "This work was sponsored by [Quansight](https://www.quansight.com/) ❤️\n",
    "\n",
    "## Create & setup a Kubernetes cluster\n",
    "\n",
    "You need to have a working kubernetes cluster that is configured correctly. If you can get\n",
    "`kubectl get ns` to work properly, it means your cluster is working fine & connected for\n",
    "this to go.\n",
    "\n",
    "## Install & run kubefwd\n",
    "\n",
    "`kubefwd` lets you access services in your cloud Kubernetes cluster as if they were\n",
    "localy, with a clever combination of ol' school `/etc/hosts` hacks & fancy kubernetes\n",
    "port-forwarding. It requires root on Mac OS & Linux, and should theoretically work on\n",
    "Windows too (haven't tested).\n",
    "\n",
    "Once you have installed it, run it in a separate terminal.\n",
    "\n",
    "```bash\n",
    "sudo kubefwd svc -n default -n kube-system\n",
    "```\n",
    "\n",
    "If you've created your own namespace for your cluster, use that instead of `default`.\n",
    "The `kube-system` is required until [this issue](https://github.com/txn2/kubefwd/issues/132)\n",
    "is fixed.\n",
    "\n",
    "If the `kubefwd` command runs successfully, we're good to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install libraries we'll need\n",
    "\n",
    "In addition to `dask-kubernetes`, we'll also need `numpy` to test our cluster with dask arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy dask distributed dask-kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup dask-kubernetes configuration\n",
    "\n",
    "Normally, the pod template would come from an external configuration file.\n",
    "We keep this in the notebook to make it more self contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POD_SPEC = {\n",
    "    'kind': 'Pod',\n",
    "    'metadata': {},\n",
    "    'spec': {\n",
    "        'restartPolicy': 'Never',\n",
    "        'containers': [\n",
    "            {\n",
    "                'image': 'daskdev/dask:latest',\n",
    "                'args': [\n",
    "                    'dask-worker',\n",
    "                    '--death-timeout', '60'\n",
    "                ],\n",
    "                'name': 'dask',\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a remote cluster & connect to it\n",
    "\n",
    "We create a `KubeCluster` object, with `deploymode='remote'`. This creates\n",
    "the scheduler as a pod on the cluster, so worker <-> scheduler communication\n",
    "is easy & efficient. `kubefwd` helps us communicate to this remote scheduler,\n",
    "so we can pretend we are actually on the remote cluster.\n",
    "\n",
    "If you are using `kubectl` to watch the objects created in your namespace,\n",
    "you'll see a `service` and a `pod` created for this. `kubefwd` should\n",
    "also list a log line about forwarding the service port locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_kubernetes import KubeCluster\n",
    "\n",
    "cluster = KubeCluster.from_dict(POD_SPEC, deploy_mode='remote')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create some workers\n",
    "\n",
    "We have a scheduler in the cloud, now time to create some workers in the\n",
    "cloud! We create 2, and can watch the worker pods come up with glee in\n",
    "`kubectl`\n",
    "\n",
    "All scaling methods (adaptive scaling, using the widget, etc) should work\n",
    "here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run some computation!\n",
    "\n",
    "We test our cluster by doing some trivial calculations with dask arrays.\n",
    "You can use any dask code as you normally would here, and it would\n",
    "run on the cloud Kubernetes cluster. This is especially helpful if you\n",
    "have large amounts of data in the cloud, since the workers would be\n",
    "really close to where the data is.\n",
    "\n",
    "You might get warnings about version mismatches. This is ok for the demo,\n",
    "in production you'd probably build your own docker image that will\n",
    "have fixed versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "import dask.array as da\n",
    "\n",
    "# Connect Dask to the cluster\n",
    "client = Client(cluster)\n",
    "\n",
    "# Create a large array and calculate the mean\n",
    "array = da.ones((1000, 1000, 1000))\n",
    "print(array.mean().compute())  # Should print 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup dask cluster\n",
    "\n",
    "When you're done with your cluster, remember to clean it up to release\n",
    "the resources!\n",
    "\n",
    "This doesn't affect your kubernetes cluster itself - you'll need to\n",
    "clean that up manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps?\n",
    "\n",
    "Lots more we can do from here.\n",
    "\n",
    "### Ephemeral Kuberentes Clusters\n",
    "\n",
    "We can wrap kubernetes cluster creation in some nice python functions, letting users create kubernetes clusters just-in-time for running a dask-kubernetes cluster, and tearing it down when they're done. Users can thus 'bring their own compute' - since the clusters will be in their cloud accounts - without having the complication of understanding how the cloud works. This is where this would be different from the wonderful [dask-gateway](https://gateway.dask.org) project I think.\n",
    "\n",
    "### Remove kubefwd\n",
    "\n",
    "`kubefwd` isn't strictly necessary, and should ideally be replaced by a `kubectl port-forward` call that doesn't require root. This should be possible with some changes to the `dask-kubernetes` code, so the client can connect to the scheduler via a different address (say, `localhost:8974`, since that's what `kubectl port-forward` gives us) vs the workers (which need something like `dask-cluster-scheduler-c12.namespace:8786`, since that is in-cluster address).\n",
    "\n",
    "Longer term, it would be great if we can get rid of spawning other processes altogether, if/when the python kubernetes client gains the [ability to port-forward](https://github.com/kubernetes-client/python/issues/166).\n",
    "\n",
    "### Integration with TLJH\n",
    "\n",
    "I love [The Littlest JupyterHub (TLJH)](https://tljh.jupyter.org). A common use case is that a group of\n",
    "users need a JupyterHub, mostly doing work that's well seved by TLJH. However, sometimes they\n",
    "need to scale up to a big dask cluster to do some work, but not for long. In these cases, I believe\n",
    "a combination of TLJH + Ephemeral Kubernetes Clusters is far simpler & easier to manage than running\n",
    "a full Kubernetes based JupyterHub. In addition, we can share the conda environment from TLJH with\n",
    "the dask workers, **removing the need for users to think about docker images or environment\n",
    "mismatches completely**. This is a massive win, and merits further exploration.\n",
    "\n",
    "### ???\n",
    "\n",
    "I am not actually an end user of dask, so I'm sure actual dask users will have much more ideas.\n",
    "Or they won't, and this will just end up being a clever hack that gives me some joy :D Who\n",
    "knows!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
